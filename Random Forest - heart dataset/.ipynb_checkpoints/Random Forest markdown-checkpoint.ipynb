{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defb28df",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "\n",
    "- **Supervised** learning model\n",
    "- Ensemble learning model used for both **classification** and **regression**\n",
    "    - **Classification**: Output is class predicted by **majority** of the trees\n",
    "    - **Regression**: Output is the **mean/average** of predictions of all individual trees\n",
    "- Random Forest Algorithm corrects the problem of **overfitting** in Decision Tree Algorithm\n",
    "\n",
    "\n",
    "## Random Forest Classifier\n",
    "\n",
    "A random forest classifier works with data having discrete labels or better known as class. \n",
    "\n",
    "_**Example**: Let us now build a Random Forest Model for say buying a car._\n",
    "\n",
    "The trees are built with a **random subset of data points** and their own set of **random features** and each tree would execute independently to provide its decision.\n",
    "\n",
    "<img src=\"example.png\" width=\"500\"/> \n",
    "\n",
    "Assuming the Decision Tree 1 suggests ‘Buy’, Decision Tree 2 Suggests ‘Don’t Buy’ and Decision Tree 3 suggests ‘Buy’, then the **max vote** would be for Buy and the result from Random Forest would be to ‘Buy’\n",
    "\n",
    "---\n",
    "We can understand this concept more with the help of a **heart related dataset**. This dataset can be found on kaggle: https://www.kaggle.com/datasets/nareshbhat/health-care-data-set-on-heart-attack-possibility\n",
    "\n",
    "But first we need to import the dependencies:\n",
    "\n",
    "### Importing dependencies\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "```\n",
    "\n",
    "This is what the dataset looks like:\n",
    "\n",
    "<img src=\"data.png\" width=\"600\"/> \n",
    "\n",
    "It consists 303 observations and 14 features. There are no null values. \n",
    "\n",
    "---\n",
    "### Classification categories\n",
    "\n",
    "> `target = 1` represents the patient is in risk of a heart attack and `target = 0` represents they are safe.</br>\n",
    "\n",
    "Lets find out the **number of observations** in each category by plotting a countplot using seaborn using the code:\n",
    "\n",
    "```python\n",
    "ax = sns.countplot(data = df, x = 'target', palette = ['green', 'orange'])\n",
    "ax.bar_label(ax.containers[0])\n",
    "```\n",
    "\n",
    "<img src=\"countplot_obs.png\" width=\"350\"/> \n",
    "\n",
    "---\n",
    "### Duplicate data points\n",
    "\n",
    "We have 1 duplicate data point in the dataset. Duplicate values can put weight on the nodes and **can make them biased**. So, its better to drop them.\n",
    "\n",
    "```python\n",
    "df.drop_duplicates(inplace = True)\n",
    "```\n",
    "\n",
    "---\n",
    "### Range of data and Outliers\n",
    "\n",
    "Now, we need to plot a **boxenplot** which will show us the range of data of each parameter and if there are any outliers.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize = (12,6))\n",
    "sns.boxenplot(data = df.drop(columns = 'target'))\n",
    "plt.xticks(rotation = 30)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<img src=\"boxenplot.png\" width=\"550\"/> \n",
    "\n",
    "#### Range of data\n",
    "\n",
    "This plot shows that the range of the data in this dataset is quite uneven. But _feature scaling_ is not required as Random Forest uses a _rule-based approach_.\n",
    "\n",
    "#### Outliers\n",
    "\n",
    "The plot indicates that there is an outlier in the _cholestrol_ feature. Usually, outliers have a negligible effect with dealing with tree-based models because the nodes are determined based on sample proportions in each split region and not on their absolute values. That being said, if the dataset is small, it might still have an impact.\n",
    "\n",
    "---\n",
    "\n",
    "### Handling the outlier\n",
    "\n",
    "We can remove the observation with this outlier with the following code:\n",
    "\n",
    "```python\n",
    "df.loc[df['chol']==df['chol'].max()]\n",
    "```\n",
    "> <img src=\"outlier.png\" width=\"550\"/> \n",
    "\n",
    "```python\n",
    "df.drop(85, axis = 0, inplace = True)\n",
    "```\n",
    "\n",
    "---\n",
    "#### Separating independent and dependent variables\n",
    "\n",
    "```python\n",
    "X = df.drop(columns = 'target')\n",
    "y = df['target']\n",
    "```\n",
    "\n",
    "---\n",
    "#### Splitting data into training and testing sets\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 8, stratify = y)\n",
    "```\n",
    "\n",
    "---\n",
    "#### Model Fitting and Training\n",
    "\n",
    "```Python\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "#### Checking the training and testing accuracy scores:\n",
    "\n",
    "```Python\n",
    "print(model.score(X_train, y_train), model.score(X_test, y_test))\n",
    "```\n",
    "> Training accuracy score : **100.0** <br> Testing accuracy score : **91.8**\n",
    "\n",
    "#### Classification Report\n",
    "\n",
    "```Python\n",
    "print(classification_report(y_test, model.predict(X_test)))\n",
    "```\n",
    "\n",
    "<img src=\"classification_report.png\" width=\"450\"/> \n",
    "\n",
    "#### Confusion matrix\n",
    "\n",
    "```Python\n",
    "cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "sns.heatmap(cm, annot = True, cmap = 'Blues')\n",
    "```\n",
    "\n",
    "<img src=\"confusion_matrix.png\" width=\"300\"/> \n",
    "\n",
    "### Summary\n",
    "\n",
    "> Random Forest uses **averaging** to improve the predictive accuracy and **control over-fitting**. <br>\n",
    "> The features are always **randomly permuted** at each split. Thus, the **best found split may vary** even with the same training data and features. <br>\n",
    "> **Normalizing of data is not required** as it uses rule-based approach. <br>\n",
    "> This algorithm is usually **robust to outliers**. <br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e2033ec4923a20e7e7bf0e28da28a2be2a649e21aef7e96b12dd75984f55e27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a950aab4",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors (KNN)\n",
    "\n",
    "- **Supervised** learning model\n",
    "- Used for both **classification** and **regression**\n",
    "- Based on **feature similarity**\n",
    "\n",
    "## KNN Classification\n",
    "\n",
    "A new datapoint is classified based on the votes of its 'k' nearest neighbors measured in Euclidean distance. \n",
    "\n",
    "Example: Here we have chosen k to be 3 and thus, based on the majority of votes, the green point will be classified as a triangle.\n",
    "\n",
    "<img src=\"example.png\" width=\"200\"/> \n",
    "\n",
    "---\n",
    "We can understand this concept more with the help of a heart related dataset. This dataset can be found on kaggle: https://www.kaggle.com/datasets/nareshbhat/health-care-data-set-on-heart-attack-possibility\n",
    "\n",
    "\n",
    "The dataset looks like this:\n",
    "\n",
    "<img src=\"data.png\" width=\"600\"/> \n",
    "\n",
    "It consists 303 observations and 14 features. There are no null values. \n",
    "> `target = 1` represents the patient is in risk of a heart attack and `target = 0` represents they are safe.\n",
    "\n",
    "---\n",
    "Lets find out the number of observations in each category by plotting a countplot using seaborn using the code:\n",
    "\n",
    "```python\n",
    "ax = sns.countplot(data = df, x = 'target', palette = 'hls')\n",
    "ax.bar_label(ax.containers[0])\n",
    "```\n",
    "\n",
    "<img src=\"countplot_obs.png\" width=\"350\"/> \n",
    "\n",
    "---\n",
    "\n",
    "### Range of data and Outliers\n",
    "\n",
    "Now, we need to plot a **boxenplot** which will show us the range of data of each parameter and if there are any outliers.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize = (12,6))\n",
    "sns.boxenplot(data = df.drop(columns = 'target'))\n",
    "plt.xticks(rotation = 30)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<img src=\"boxenplot.png\" width=\"550\"/> \n",
    "\n",
    "This plot shows that the range of the data in this dataset is quite uneven and there is an outlier in the cholestrol feature but in case of medical records, removing an outlier observation is not a good idea.\n",
    "\n",
    "---\n",
    "### Scaling the data\n",
    "\n",
    "Since KNN algorithm, relies on distance for classification if the features show a vast difference in range of data (eg. age and cholestrol) sometimes due to their units, then normalizing and scaling the training data can improve accuracy dramatically.\n",
    "\n",
    "```python\n",
    "X = df.drop(columns = 'target')\n",
    "y = df['target']\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "X_std = pd.DataFrame(X_std, columns = list(X.columns))\n",
    "```\n",
    "\n",
    "Now our boxenplot looks like this:\n",
    "\n",
    "<img src=\"boxenplot_scaled.png\" width=\"550\"/> \n",
    "\n",
    "---\n",
    "#### Splitting data into training and testing sets\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.2, stratify = y, random_state = 14)\n",
    "```\n",
    "---\n",
    "\n",
    "### Value of k\n",
    "\n",
    "- As k is the number of datapoint neighbors, it can only be a **positive integer**.\n",
    "- If `k = 1`, then the new datapoint will be assigned to the class of that of the single nearest neighbor. \n",
    "- If `k = 11`, then we use the vote of 11 nearest neighbors to determine the class of our datapoint.\n",
    "- The value of k should neither be too high nor too low.\n",
    "\n",
    "#### Choosing the right value\n",
    "\n",
    "Picking out the right value for k comes under a process called **parameter tuning** and is important for better accuracy.\n",
    "To get it right, we need to run the algorithm several times with different values of k and choose the one with less errors and better accuracy.\n",
    "\n",
    "We will accomplish this by the following code:\n",
    "\n",
    "```python\n",
    "train_score = []\n",
    "test_score = []\n",
    "k_value = []\n",
    "\n",
    "for k in range(1,26):     \n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)    \n",
    "    k_value.append(k)\n",
    "    train_score.append(knn.score(X_train, y_train))\n",
    "    test_score.append(knn.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "We have the training and testing scores for every k value ranging from **1 to 25**. We need to plot a graph to get a better look.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(k_value, train_score, label = 'Training Accuracy')\n",
    "plt.plot(k_value, test_score, label = 'Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Graph for different values of k')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy Score (Train and Test)')\n",
    "plt.xticks(k_value)\n",
    "plt.grid(linestyle = '--')\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"choice_of_k.png\" width=\"600\"/> \n",
    "\n",
    "Best accuracy is given by `k=7`\n",
    "\n",
    "---\n",
    "#### Model Fitting and Training\n",
    "\n",
    "```Python\n",
    "knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "knn.fit(X_train, y_train)\n",
    "```\n",
    "#### Checking the training and testing scores at `k=7`:\n",
    "\n",
    "```Python\n",
    "print(knn.score(X_train, y_train), knn.score(X_test, y_test))\n",
    "```\n",
    "> Training accuracy score : 87.2 <br> Testing accuracy score : 93.4\n",
    "\n",
    "#### Classification Report\n",
    "\n",
    "```Python\n",
    "print(classification_report(y_test, knn.predict(X_test)))\n",
    "```\n",
    "\n",
    "<img src=\"classification_report.png\" width=\"450\"/> \n",
    "\n",
    "#### Confusion matrix\n",
    "\n",
    "```Python\n",
    "cm = confusion_matrix(y_test, knn.predict(X_test))\n",
    "sns.heatmap(cm, annot = True, cmap = 'Blues')\n",
    "```\n",
    "\n",
    "<img src=\"confusion_matrix.png\" width=\"300\"/> \n",
    "\n",
    "### Summary\n",
    "\n",
    "> KNN is a **quick**, **simple** and **accurate model** if given a right quality dataset. <br>\n",
    "> It works better with **smaller** and **noise-free** datasets. <br>\n",
    "> It tends to be sensitive to the **scale of data**. <br>\n",
    "> **Optimal value of k** can be found by running the algorithms several times with different values.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
